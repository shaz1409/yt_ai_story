# Full Pipeline Implementation Summary

## Overview

The "last mile" of the AI Story Shorts Factory pipeline has been implemented, completing the end-to-end flow from topic to YouTube upload.

## New Components

### 1. Video Renderer (`app/services/video_renderer.py`)

**Purpose**: Renders `VideoPlan` into final vertical .mp4 video

**Features**:
- Generates narration audio using TTS (ElevenLabs, OpenAI, or stub)
- Generates scene visuals (Hugging Face API or placeholder images)
- Composes vertical 1080x1920 .mp4 video using MoviePy
- Allocates scene durations based on narration lines
- Comprehensive logging at each step

**Key Methods**:
- `render(video_plan, output_dir)` - Main entrypoint
- `_generate_narration_audio()` - TTS integration
- `_generate_scene_visuals()` - Image generation
- `_compose_video()` - Video composition

### 2. TTS Client (`app/services/tts_client.py`)

**Purpose**: Abstract TTS client supporting multiple providers

**Features**:
- Auto-detects available provider (ElevenLabs → OpenAI → Stub)
- Clean interface for easy provider swapping
- Stub mode with fallback (pydub or wave module)

**Supported Providers**:
- ElevenLabs (via API)
- OpenAI TTS (via API)
- Stub (silent audio for testing)

### 3. YouTube Uploader (`app/services/youtube_uploader.py`)

**Purpose**: Uploads videos to YouTube via Data API v3

**Features**:
- OAuth 2.0 flow with token caching
- Resumable uploads with progress logging
- Sets title, description, tags, privacy status
- Returns YouTube video URL

**Configuration**:
- Requires `client_secrets.json` from Google Cloud Console
- Token cached in `youtube_token.json`
- First run opens browser for OAuth consent

### 4. Full Pipeline Orchestrator (`run_full_pipeline.py`)

**Purpose**: CLI script that wires everything together

**Pipeline Flow**:
1. Story Generation → VideoPlan
2. Video Rendering → .mp4 file
3. YouTube Upload (optional) → YouTube URL

**Usage**:
```bash
python run_full_pipeline.py \
  --topic "courtroom drama – teen laughs at verdict" \
  --duration-target-seconds 60 \
  --auto-upload \
  --output-dir outputs/videos
```

## Configuration

### Required Environment Variables

```bash
# TTS (choose one)
ELEVENLABS_API_KEY=your_key
ELEVENLABS_VOICE_ID=your_voice_id
# OR
OPENAI_API_KEY=your_key

# YouTube Upload (optional)
YOUTUBE_CLIENT_SECRETS_FILE=path/to/client_secrets.json
YOUTUBE_TOKEN_FILE=youtube_token.json

# Image Generation (optional)
HUGGINGFACE_TOKEN=your_token
```

## Testing

### Test Files Created

1. `tests/test_video_renderer.py` - Video rendering tests
2. `tests/test_youtube_uploader.py` - YouTube upload tests (mocked)
3. `tests/test_run_full_pipeline.py` - End-to-end pipeline tests (mocked)

### Running Tests

```bash
pytest tests/test_video_renderer.py
pytest tests/test_youtube_uploader.py
pytest tests/test_run_full_pipeline.py
```

## Dependencies Added

Updated `requirements_backend.txt` with:
- `pydub>=0.25.1` - Stub TTS audio generation
- `google-auth>=2.23.0` - YouTube OAuth
- `google-auth-oauthlib>=1.1.0` - OAuth flow
- `google-auth-httplib2>=0.1.1` - OAuth HTTP
- `google-api-python-client>=2.100.0` - YouTube Data API

## Architecture

```
Topic
  ↓
Story Generation (existing services)
  ↓
VideoPlan
  ↓
VideoRenderer
  ├─ TTS Client → Narration Audio
  ├─ CharacterVideoEngine → Character Faces + Talking-Head Clips
  ├─ Image Generator → Scene Visuals
  └─ MoviePy → Final .mp4 (with talking-heads inserted)
  ↓
YouTubeUploader (optional)
  ↓
YouTube Video URL
```

## Key Design Decisions

1. **Modular Services**: Each component is independently testable
2. **Provider Abstraction**: TTS and image generation use clean interfaces
3. **Graceful Degradation**: Stub modes when APIs unavailable
4. **Comprehensive Logging**: Every step is logged with context
5. **Error Handling**: Proper exceptions with clear messages

## Talking-Head Character Animations (V1)

### Overview

The pipeline now supports **photoreal character talking-head clips** that appear during key dialogue lines, while the narrator handles most of the storytelling.

### How It Works

1. **Character Face Generation**: For each non-narrator character, a photorealistic base face image is generated using the same image generation stack (Hugging Face / Stable Diffusion).

2. **Dialogue Selection**: The system automatically selects the top N (default: 3) most emotional/important dialogue lines based on:
   - Emotion tags (rage, shock, angry score highest)
   - Scene importance (conflict/twist scenes prioritized)

3. **Dialogue Audio**: Each selected dialogue line gets its own TTS audio using the character's voice profile.

4. **Talking-Head Clips**: Short video clips are generated by:
   - Taking the character's base face image
   - Adding subtle zoom/pan animation
   - Syncing with the dialogue audio
   - Creating a vertical 1080x1920 clip

5. **Video Composition**: Talking-head clips are inserted into the video timeline at the appropriate scenes, alternating with scene visuals.

### Architecture

```
CharacterVideoEngine
  ├─ generate_character_face_image() → Photoreal portrait
  ├─ generate_talking_head_clip() → Animated clip
  └─ TalkingHeadProvider (pluggable)
      └─ Currently: Static image + zoom + audio
      └─ Future: Real lip-sync API (D-ID, HeyGen, etc.)
```

### Configuration

**Environment Variables**:
```bash
# Enable/disable talking heads (default: true)
USE_TALKING_HEADS=true
MAX_TALKING_HEAD_LINES_PER_VIDEO=3
```

**CLI Flags**:
```bash
# Disable talking heads
python run_full_pipeline.py --topic "..." --no-talking-heads

# Set max animated lines
python run_full_pipeline.py --topic "..." --max-talking-head-lines 5
```

### Usage Example

```bash
python run_full_pipeline.py \
  --topic "teen laughs in court after verdict" \
  --style "courtroom_drama" \
  --auto-upload
```

By default, this will:
- Generate character face images for judge, defendant, etc.
- Select top 3 emotional dialogue lines
- Create talking-head clips for those lines
- Insert them into the final video

### Current Implementation (V1)

**What works**:
- ✅ Photoreal character face generation
- ✅ Per-character voice profiles for dialogue
- ✅ Automatic selection of key dialogue lines
- ✅ Talking-head clips with subtle animation
- ✅ Integration into video timeline

**Limitations (by design)**:
- Talking-head clips use static image + zoom (no real lip-sync yet)
- Architecture is ready for real talking-head API integration
- Can be swapped without changing VideoRenderer logic

### Future Enhancements

1. **Real Lip-Sync**: Integrate D-ID, HeyGen, or custom model for actual mouth movement
2. **More Animation**: Add head movements, expressions, eye blinks
3. **Better Selection**: ML-based selection of most impactful dialogue lines
4. **Character Consistency**: Ensure same character face across multiple clips

## Next Steps

1. **Enhanced Visuals**: Improve image prompts and generation quality
2. **Real Lip-Sync**: Integrate actual talking-head API for mouth movement
3. **Video Effects**: Add transitions, animations, text overlays
4. **Batch Processing**: Generate multiple videos in parallel
5. **Quality Checks**: Validate video before upload

## Files Modified/Created

### New Files
- `app/services/video_renderer.py`
- `app/services/tts_client.py`
- `app/services/youtube_uploader.py`
- `app/services/character_video_engine.py` - Character face generation and talking-head clips
- `run_full_pipeline.py`
- `tests/test_video_renderer.py`
- `tests/test_youtube_uploader.py`
- `tests/test_run_full_pipeline.py`
- `tests/test_character_video_engine.py`

### Modified Files
- `app/core/config.py` - Added YouTube and TTS settings
- `requirements_backend.txt` - Added new dependencies
- `README_BACKEND.md` - Added full pipeline documentation

## Status

✅ **Complete**: All components implemented, tested, and documented

The full pipeline is production-ready and can generate videos from topics and upload them to YouTube with a single command.

